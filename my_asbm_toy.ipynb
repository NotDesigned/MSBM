{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c11eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import random\n",
    "\n",
    "# from posteriors import Diffusion_Coefficients, BrownianPosterior_Coefficients, get_time_schedule\n",
    "from sampling_utils import extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f632682a",
   "metadata": {},
   "source": [
    "ASBM训练:\n",
    "\n",
    "$x_0 \\sim p_{data}$\n",
    "\n",
    "$x_1 \\sim p_{prior}$\n",
    "\n",
    "$\\varepsilon$ 是布朗桥强度\n",
    "\n",
    "整体上分为两步\n",
    "\n",
    "1.从$x_0$和$x_1$的联合分布中采样$(x_0, x_1)$, 然后得出条件化的布朗桥\n",
    "$$\n",
    "p^{W^\\varepsilon}(x_{t_1}\\ldots x_{t_N} | x_0, x_1) = \\prod_{i=1}^{N} p^{W^\\varepsilon}(x_{t_i} | x_{t_{i-1}}, x_1) \\\\\n",
    "p^{W^\\varepsilon}(x_{t_i} | x_{t_{i-1}}, x_1) = \\mathcal{N}(x_{t_i}; x_{t_{i-1}} + \\frac{t_i - t_{i-1}}{1 - t_{i-1}}(x_1 - x_{t_{i-1}}),\\varepsilon \\frac{(t_i - t_{i-1})(1 - t_i)}{(1 - t_{i-1})} I)\\\\\n",
    "p^{W^\\varepsilon}(x_{t_{i}} | x_{t_{i+1}}, x_1) = \\mathcal{N}(x_{t_i}; \\frac{t_{i+1}-t_{i}}{t_{i+1}} x_0 + \\frac{t_{i}}{t_{i+1}} x_{t_{i+1}},\\varepsilon \\frac{t_i(t_{i+1} - t_{i})}{t_{i+1}} I)\\\\\n",
    "p^{W^\\varepsilon}(x_{t_N} | x_0, x_1) = \\mathcal{N}(x_{t_N}; (1 - t_N) x_0 + t_N x_1, \\varepsilon t_N(1 - t_N) I)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b35eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrownianPosterior_Coefficients():\n",
    "    def __init__(self, args, device):\n",
    "        epsilon = args.epsilon\n",
    "        self.epsilon = epsilon\n",
    "        print(f\"BrownianPosterior with epsilon {epsilon} and num steps {args.num_timesteps}\")\n",
    "        num_timesteps = args.num_timesteps\n",
    "\n",
    "        t = torch.linspace(0, 1, num_timesteps+1, device=device)\n",
    "        self.posterior_mean_coef1 = 1 - t[:-1]/t[1:]\n",
    "        self.posterior_mean_coef2 = t[:-1]/t[1:]\n",
    "\n",
    "        self.posterior_variance = epsilon*t[:-1]*(t[1:] - t[:-1])/t[1:]\n",
    "        self.posterior_log_variance_clipped = torch.log(self.posterior_variance.clamp(min=1e-20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "class Checkerboard(Dataset):\n",
    "    def __init__(self, size=8, grid_size=4):\n",
    "        self.size = size\n",
    "        self.grid_size = grid_size\n",
    "        self.checkboard = torch.tensor([[i, j] for i in range(grid_size) for j in range(grid_size) if (i + j) % 2 == 0])\n",
    "\n",
    "        grid_pos = torch.randint(low=0, high=self.checkboard.shape[0], size=(self.size,), dtype=torch.int64)\n",
    "        self.data = torch.rand(size=(self.size, 2), dtype=torch.float32) + self.checkboard[grid_pos].float()\n",
    "        self.data = self.data / self.grid_size * 2 - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class Pinwheel(Dataset):\n",
    "    def __init__(self, npar: int):\n",
    "        self.size = npar\n",
    "\n",
    "        radial_std = 0.3\n",
    "        tangential_std = 0.1\n",
    "        num_classes = 7\n",
    "        num_per_class = math.ceil(npar / num_classes)\n",
    "        rate = 0.25\n",
    "        rads = np.linspace(0, 2 * np.pi, num_classes, endpoint=False)\n",
    "\n",
    "        features = np.random.randn(num_classes*num_per_class, 2) \\\n",
    "            * np.array([radial_std, tangential_std])\n",
    "        features[:, 0] += 1.\n",
    "        labels = np.repeat(np.arange(num_classes), num_per_class)\n",
    "\n",
    "        angles = rads[labels] + rate * np.exp(features[:, 0])\n",
    "        rotations = np.stack([np.cos(angles), -np.sin(angles), np.sin(angles), np.cos(angles)])\n",
    "        rotations = np.reshape(rotations.T, (-1, 2, 2))\n",
    "        x = .4 * np.random.permutation(np.einsum(\"ti,tij->tj\", features, rotations))\n",
    "\n",
    "        self.init_sample = torch.from_numpy(x).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tensor:\n",
    "        return self.init_sample[idx]\n",
    "\n",
    "\n",
    "data_size = 2 ** 20\n",
    "pinwheel_dataset = Pinwheel(data_size)\n",
    "checkerboard_dataset = Checkerboard(size=data_size, grid_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6656824",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 ** 10\n",
    "pinwheel_data_loader = DataLoader(pinwheel_dataset, batch_size, num_workers=0, pin_memory=True, shuffle=True)\n",
    "checkerboard_data_loader = DataLoader(checkerboard_dataset, batch_size, num_workers=0, pin_memory=True, shuffle=True)\n",
    "\n",
    "def show_2d_data(data: Tensor):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.scatter(data[:, 0], data[:, 1])\n",
    "    plt.xlim(-1.1, 1.1)\n",
    "    plt.ylim(-1.1, 1.1)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c6778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinwheel_batch = next(iter(pinwheel_data_loader))\n",
    "checkerboard_batch = next(iter(checkerboard_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_2d_data(next(iter(pinwheel_data_loader)))\n",
    "show_2d_data(next(iter(checkerboard_data_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1943ba",
   "metadata": {},
   "source": [
    "接下来需要一个统一的DataLoader，返回一个Batch的(x,y)配对数据，作为从联合分布q采样的工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3929f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    为 SB 训练提供成对样本 (x0, x1)：\n",
    "      x0 ~ target_ds (p0),  x1 ~ source_ds (p1)\n",
    "    pairing:\n",
    "      - \"independent\": 每次各自随机取一个（独立耦合，默认）\n",
    "      - \"aligned\":     用相同索引配对（便于可复现/调试）\n",
    "    \"\"\"\n",
    "    def __init__(self, target_ds: Dataset, source_ds: Dataset, pairing: str = \"independent\"):\n",
    "        self.target_ds = target_ds\n",
    "        self.source_ds = source_ds\n",
    "        self.pairing = pairing\n",
    "        self.len = max(len(target_ds), len(source_ds))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.pairing == \"aligned\":\n",
    "            i0 = idx % len(self.target_ds)\n",
    "            i1 = idx % len(self.source_ds)\n",
    "        else:  # independent\n",
    "            i0 = random.randint(0, len(self.target_ds) - 1)\n",
    "            i1 = random.randint(0, len(self.source_ds) - 1)\n",
    "        x0 = self.target_ds[i0]   # Tensor [..., 2]\n",
    "        x1 = self.source_ds[i1]   # Tensor [..., 2]\n",
    "        return x0, x1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9d5546",
   "metadata": {},
   "source": [
    "接下来的训练，我们要能做到以下几个能力\n",
    "1. 给定$x_0, x_1\\sim q(x_0,x_1)$ （这里直接假设独立）, 构造 $(x_t, x_{t+1})$ 作为布朗桥ground truth, 先按$q(x_{t+1}| x_0, x_1)$采样$x_{t+1}$，再按$q(x_t|x_{t+1}, x_0)$采样$x_{t}$\n",
    "2. 生成器：给定$(x_t,t)$， 预测 $x_0$\n",
    "3. 判别器：给定$(x_t, x_{t+1})$，判断是不是由生成器生成的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfd90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_posterior(coefficients, x0, x_tp1, t_idx):\n",
    "    \"\"\"\n",
    "    Brownian-bridge one-step posterior:\n",
    "    Sample x_t ~ q(x_t | x_{t+1}=x_tp1, x_0=x0) for discrete index t_idx in {0,...,N-1}.\n",
    "    coefficients: BrownianPosterior_Coefficients (has posterior_mean_coef1/2, posterior_log_variance_clipped)\n",
    "    x0, x_tp1: (B, D)\n",
    "    t_idx: (B,) int64\n",
    "    \"\"\"\n",
    "    assert t_idx.dtype in (torch.int32, torch.int64), \"t_idx must be integer indices\"\n",
    "    a_t = extract(coefficients.posterior_mean_coef1, t_idx, x_tp1.shape)  # coef for x0\n",
    "    b_t = extract(coefficients.posterior_mean_coef2, t_idx, x_tp1.shape)  # coef for x_{t+1}\n",
    "    mean = a_t * x0 + b_t * x_tp1\n",
    "    log_var = extract(coefficients.posterior_log_variance_clipped, t_idx, x_tp1.shape)\n",
    "\n",
    "    noise = torch.randn_like(x_tp1)\n",
    "    nonzero_mask = (1 - (t_idx == 0).float())\n",
    "    while len(nonzero_mask.shape) < len(mean.shape):\n",
    "        nonzero_mask = nonzero_mask.unsqueeze(-1)\n",
    "    return mean + nonzero_mask * torch.exp(0.5 * log_var) * noise\n",
    "\n",
    "\n",
    "def sample_bridge(coefficients, x0, x1, tau):\n",
    "    \"\"\"\n",
    "    Brownian bridge marginal:\n",
    "    Sample X_tau ~ N( (1-tau)x0 + tau x1,  epsilon * tau * (1 - tau) I ), where tau in [0,1].\n",
    "    tau: (B,1) or (B,) float in [0,1]\n",
    "    \"\"\"\n",
    "    if tau.ndim == 1:\n",
    "        tau = tau.unsqueeze(-1)\n",
    "    assert tau.dtype.is_floating_point, \"tau must be float in [0,1]\"\n",
    "    mean = (1. - tau) * x0 + tau * x1\n",
    "    var = coefficients.epsilon * tau * (1. - tau)\n",
    "    log_var = torch.log(var.clamp_min(1e-20))\n",
    "    noise = torch.randn_like(x0)\n",
    "\n",
    "    # turn off noise exactly at endpoints tau=0 or tau=1\n",
    "    nonzero_mask = (1. - (tau.eq(0.).float())) * (1. - (tau.eq(1.).float()))\n",
    "    return mean + nonzero_mask * torch.exp(0.5 * log_var) * noise\n",
    "\n",
    "\n",
    "def sample_pair_condition(coefficients, x0, x1, t_idx):\n",
    "    \"\"\"\n",
    "    Build a real pair (x_t, x_{t+1}) given endpoints (x0, x1) and discrete index t_idx in {0,...,N-1}.\n",
    "    Steps:\n",
    "      1) tau_{t+1} = (t_idx+1)/N, sample x_{t+1} from bridge marginal\n",
    "      2) sample x_t from one-step posterior q(x_t | x_{t+1}, x0)\n",
    "    \"\"\"\n",
    "    assert t_idx.dtype in (torch.int32, torch.int64)\n",
    "    N = coefficients.posterior_mean_coef1.shape[0]  # N steps\n",
    "    tau_tp1 = (t_idx.to(torch.float32) + 1.) / float(N)  # (B,)\n",
    "    x_tp1 = sample_bridge(coefficients, x0, x1, tau_tp1)           # (B, D)\n",
    "    x_t   = sample_posterior(coefficients, x0, x_tp1, t_idx)       # (B, D)\n",
    "    return x_t, x_tp1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb039d42",
   "metadata": {},
   "source": [
    "现在是判别器和生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self, x_dim, t_dim, n_t, z_dim, out_dim, layers=[128, 128, 128],\n",
    "        active=partial(nn.LeakyReLU, 0.2),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.x_dim = x_dim\n",
    "        self.t_dim = t_dim\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        self.model_list = []\n",
    "        ch_prev = x_dim + t_dim + z_dim\n",
    "\n",
    "        self.t_transform = nn.Embedding(n_t, t_dim,)\n",
    "\n",
    "        for ch_next in layers:\n",
    "            self.model_list.append(nn.Linear(ch_prev, ch_next))\n",
    "            self.model_list.append(active())\n",
    "            ch_prev = ch_next\n",
    "\n",
    "        self.model_list.append(nn.Linear(ch_prev, out_dim))\n",
    "        self.model = nn.Sequential(*self.model_list)\n",
    "\n",
    "    def forward(self, x, t, z):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        if z.shape != (batch_size, self.z_dim):\n",
    "            z = z.reshape((batch_size, self.z_dim))\n",
    "\n",
    "        return self.model(\n",
    "            torch.cat([\n",
    "                x,\n",
    "                self.t_transform(t),\n",
    "                z,\n",
    "            ], dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "class MyDiscriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self, x_dim, t_dim, n_t, layers=[128, 128, 128],\n",
    "        active=partial(nn.LeakyReLU, 0.2),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.x_dim = x_dim\n",
    "        self.t_dim = t_dim\n",
    "\n",
    "        self.model_list = []\n",
    "        ch_prev = 2 * x_dim + t_dim\n",
    "\n",
    "        self.t_transform = nn.Embedding(n_t, t_dim,)\n",
    "        \n",
    "        for ch_next in layers:\n",
    "            # print(f\"Build layer from {ch_prev} to {ch_next}\")\n",
    "            self.model_list.append(nn.Linear(ch_prev, ch_next))\n",
    "            self.model_list.append(active())\n",
    "            ch_prev = ch_next\n",
    "\n",
    "        self.model_list.append(nn.Linear(ch_prev, 1))\n",
    "        self.model = nn.Sequential(*self.model_list)\n",
    "\n",
    "    def forward(self, x_t, t, x_tp1,):\n",
    "        transform_t = self.t_transform(t)\n",
    "        # print(f\"x_t.shape = {x_t.shape}, transform_t = {transform_t.shape}, x_tp1 = {x_tp1.shape}\")\n",
    "\n",
    "        return self.model(\n",
    "            torch.cat([\n",
    "                x_t,\n",
    "                transform_t,\n",
    "                x_tp1,\n",
    "            ], dim=1)\n",
    "        ).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de16e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c5bef8",
   "metadata": {},
   "source": [
    "必要的参数\n",
    "1. Batch size\n",
    "2. learning rate: lr_d, lr_g\n",
    "3. optimizer: Adam beta1, beta2\n",
    "4. epoch\n",
    "5. epsilon\n",
    "6. x_dim, 数据维度\n",
    "7. t_dim, 时间维度\n",
    "8. z_dim, 隐空间维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa44ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def parse_args(usedefault:bool = False):\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch_size', type=int, default=2**12)\n",
    "    parser.add_argument('--lr_d', type=float, default=2e-4)\n",
    "    parser.add_argument('--lr_g', type=float, default=2e-4)\n",
    "    parser.add_argument('--beta1', type=float, default=0.5)\n",
    "    parser.add_argument('--beta2', type=float, default=0.9)\n",
    "    parser.add_argument('--epoch', type=int, default=50)\n",
    "    parser.add_argument('--epsilon', type=float, default=1.0)\n",
    "    parser.add_argument('--x_dim', type=int, default=2)\n",
    "    parser.add_argument('--t_dim', type=int, default=6)\n",
    "    parser.add_argument('--z_dim', type=int, default=2)\n",
    "    parser.add_argument('--num_timesteps', type=int, default=10)\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--output_dir', type=str, default='./output')\n",
    "    parser.add_argument('--use_minibatch_ot', action='store_true', help='whether to use minibatch OT coupling')\n",
    "    if usedefault:\n",
    "        args = parser.parse_args([])\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def train(args):\n",
    "    set_seed(args.seed)\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    # ==== Brownian posterior coefficients ====\n",
    "    coefficients = BrownianPosterior_Coefficients(args, device)\n",
    "    \n",
    "    # ==== Models ====\n",
    "    netG = MyGenerator(\n",
    "        x_dim=args.x_dim,\n",
    "        t_dim=args.t_dim,\n",
    "        n_t=args.num_timesteps,\n",
    "        z_dim=args.z_dim,\n",
    "        out_dim=args.x_dim,\n",
    "        layers=[128, 128, 128],\n",
    "    ).to(device)\n",
    "    netD = MyDiscriminator(\n",
    "        x_dim=args.x_dim,\n",
    "        t_dim=args.t_dim,\n",
    "        n_t=args.num_timesteps,\n",
    "        layers=[128, 128, 128],\n",
    "    ).to(device)\n",
    "\n",
    "    # ==== Opt & Sched ====\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr_d, betas=(args.beta1, args.beta2))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=args.lr_g, betas=(args.beta1, args.beta2))\n",
    "    schedulerD = optim.lr_scheduler.CosineAnnealingLR(optimizerD, T_max=args.epoch, eta_min=1e-6)\n",
    "    schedulerG = optim.lr_scheduler.CosineAnnealingLR(optimizerG, T_max=args.epoch, eta_min=1e-6)\n",
    "\n",
    "    # ==== (Optional) resume ====\n",
    "    init_iteration = 0\n",
    "    checkpoint_path = os.path.join(args.output_dir, 'checkpoint.pth')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        netG.load_state_dict(checkpoint['netG_dict'])\n",
    "        optimizerG.load_state_dict(checkpoint['optimizerG'])\n",
    "        schedulerG.load_state_dict(checkpoint['schedulerG'])\n",
    "        netD.load_state_dict(checkpoint['netD_dict'])\n",
    "        optimizerD.load_state_dict(checkpoint['optimizerD'])\n",
    "        schedulerD.load_state_dict(checkpoint['schedulerD'])\n",
    "        init_iteration = checkpoint['iteration']\n",
    "        print(f\"=> loaded checkpoint (iteration {init_iteration})\")\n",
    "\n",
    "    # ==== Paired dataloader for SB ====\n",
    "    paired_ds = PairedDataset(target_ds=checkerboard_dataset, source_ds=pinwheel_dataset, pairing=\"independent\")\n",
    "    sb_loader = DataLoader(\n",
    "        paired_ds, batch_size=args.batch_size, shuffle=True,\n",
    "        num_workers=0, pin_memory=True, drop_last=True\n",
    "    )\n",
    "    n_t = args.num_timesteps\n",
    "    # ==== utils ====\n",
    "    softplus = F.softplus\n",
    "\n",
    "    def r1_penalty(d_out, x_in, gamma=0.05):\n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=d_out.sum(), inputs=x_in, create_graph=True,\n",
    "            retain_graph=True, only_inputs=True\n",
    "        )[0]\n",
    "        grad_penalty = (grad.view(grad.size(0), -1).norm(2, dim=1) ** 2).mean()\n",
    "        return 0.5 * gamma * grad_penalty\n",
    "\n",
    "    # ==== Train loop ====\n",
    "    iteration = init_iteration\n",
    "    print(\"Start training ASBM...\")\n",
    "    for epoch in range(args.epoch):\n",
    "        for x0, x1 in sb_loader:\n",
    "            netG.train(); netD.train()\n",
    "            x0 = x0.to(device, non_blocking=True)   # 目标端点 p0（checkerboard）\n",
    "            x1 = x1.to(device, non_blocking=True)   # 源端点 p1（pinwheel）\n",
    "            B  = x0.size(0)\n",
    "\n",
    "            # （可选）批内 OT 重配\n",
    "            if args.use_minibatch_ot:\n",
    "                # TODO\n",
    "                pass\n",
    "\n",
    "            # 采离散时间索引\n",
    "            t_idx = torch.randint(0, n_t, (B,), device=device, dtype=torch.long)\n",
    "\n",
    "            # ---------- D step ----------\n",
    "            for p in netD.parameters(): p.requires_grad = True\n",
    "            optimizerD.zero_grad()\n",
    "\n",
    "            # 真对（互反投影：布朗桥条件）\n",
    "            x_t_real, x_tp1_real = sample_pair_condition(coefficients, x0, x1, t_idx)\n",
    "            x_t_real.requires_grad_(True)\n",
    "            D_real = netD(x_t_real, t_idx, x_tp1_real.detach())\n",
    "            errD_real = softplus(-D_real).mean()\n",
    "            r1 = r1_penalty(D_real, x_t_real, gamma=0.05)\n",
    "\n",
    "            # 假对（G 预测端点 + 后验回退）\n",
    "            z = torch.randn(B, args.z_dim, device=device)\n",
    "            x0_pred = netG(x_tp1_real.detach(), t_idx, z)\n",
    "            x_t_fake = sample_posterior(coefficients, x0_pred, x_tp1_real.detach(), t_idx)\n",
    "            D_fake = netD(x_t_fake.detach(), t_idx, x_tp1_real.detach())\n",
    "            errD_fake = softplus(D_fake).mean()\n",
    "\n",
    "            errD = errD_real + errD_fake + r1\n",
    "            errD.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "            # ---------- G step ----------\n",
    "            for p in netD.parameters(): p.requires_grad = False\n",
    "            optimizerG.zero_grad()\n",
    "\n",
    "            # 重新构造一批 x_{t+1}（或复用上面的亦可）\n",
    "            t_idx = torch.randint(0, n_t, (B,), device=device, dtype=torch.long)\n",
    "            _, x_tp1 = sample_pair_condition(coefficients, x0, x1, t_idx)\n",
    "            z = torch.randn(B, args.z_dim, device=device)\n",
    "            x0_pred = netG(x_tp1, t_idx, z)\n",
    "            x_t_fake = sample_posterior(coefficients, x0_pred, x_tp1, t_idx)\n",
    "            D_fake = netD(x_t_fake, t_idx, x_tp1)\n",
    "            errG = softplus(-D_fake).mean()\n",
    "            errG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            iteration += 1\n",
    "            if iteration % 200 == 0:\n",
    "                print(f\"[Iter {iteration}] D={errD.item():.4f} (real {errD_real.item():.4f} fake {errD_fake.item():.4f} r1 {r1.item():.4f}) | G={errG.item():.4f}\")\n",
    "\n",
    "            # 可视化（从源出发逆推到目标）\n",
    "            if iteration % 2000 == 0:\n",
    "                with torch.no_grad():\n",
    "                    x_vis = x1[:4096].clone().to(device)\n",
    "                    # Plot subplots for every timesteps\n",
    "                    fig, axes = plt.subplots(1, n_t+1, figsize=((n_t+1)*2, 3))\n",
    "                    axes[0].scatter(x_vis[:,0].cpu(), x_vis[:,1].cpu(), s=2)\n",
    "                    axes[0].set_title(\"t=1.0\")\n",
    "                    for i in reversed(range(n_t)):\n",
    "                        t_vis = torch.full((x_vis.size(0),), i, device=device, dtype=torch.long)\n",
    "                        z_vis = torch.randn(x_vis.size(0), args.z_dim, device=device)\n",
    "                        x0_pred = netG(x_vis, t_vis, z_vis)\n",
    "                        x_vis = sample_posterior(coefficients, x0_pred, x_vis, t_vis)\n",
    "                        id = n_t - i\n",
    "                        axes[id].scatter(x_vis[:,0].cpu(), x_vis[:,1].cpu(), s=2)\n",
    "                        axes[id].set_title(f\"{t/n_t}\")\n",
    "                        axes[id].set_xlim(-1.1,1.1); axes[id].set_ylim(-1.1,1.1)\n",
    "                    plt.suptitle(f\"iter {iteration}\")\n",
    "                    plt.show(); plt.close()\n",
    "\n",
    "\n",
    "        # 每个 epoch 调度 & 保存\n",
    "        schedulerD.step(); schedulerG.step()\n",
    "        torch.save({\n",
    "            'iteration': iteration,\n",
    "            'netG_dict': netG.state_dict(),\n",
    "            'optimizerG': optimizerG.state_dict(),\n",
    "            'schedulerG': schedulerG.state_dict(),\n",
    "            'netD_dict': netD.state_dict(),\n",
    "            'optimizerD': optimizerD.state_dict(),\n",
    "            'schedulerD': schedulerD.state_dict(),\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved @ epoch {epoch+1}\")\n",
    "\n",
    "\n",
    "args = parse_args(usedefault=True)\n",
    "print(args)\n",
    "train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSBM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
