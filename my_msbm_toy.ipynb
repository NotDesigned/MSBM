{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c11eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import random\n",
    "\n",
    "# from posteriors import Diffusion_Coefficients, BrownianPosterior_Coefficients, get_time_schedule\n",
    "from typing import List, Optional, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b6601",
   "metadata": {},
   "source": [
    "toy数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8bdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "class Checkerboard(Dataset):\n",
    "    def __init__(self, size=8, grid_size=4):\n",
    "        self.size = size\n",
    "        self.grid_size = grid_size\n",
    "        self.checkboard = torch.tensor([[i, j] for i in range(grid_size) for j in range(grid_size) if (i + j) % 2 == 0])\n",
    "\n",
    "        grid_pos = torch.randint(low=0, high=self.checkboard.shape[0], size=(self.size,), dtype=torch.int64)\n",
    "        self.data = torch.rand(size=(self.size, 2), dtype=torch.float32) + self.checkboard[grid_pos].float()\n",
    "        self.data = self.data / self.grid_size * 2 - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class Pinwheel(Dataset):\n",
    "    def __init__(self, size: int, num_classes:int=8):\n",
    "        self.size = size\n",
    "\n",
    "        radial_std = 0.3\n",
    "        tangential_std = 0.1\n",
    "        num_per_class = math.ceil(size / num_classes)\n",
    "        rate = 0.25\n",
    "        rads = np.linspace(0, 2 * np.pi, num_classes, endpoint=False)\n",
    "\n",
    "        features = np.random.randn(num_classes*num_per_class, 2) \\\n",
    "            * np.array([radial_std, tangential_std])\n",
    "        features[:, 0] += 1.\n",
    "        labels = np.repeat(np.arange(num_classes), num_per_class)\n",
    "\n",
    "        angles = rads[labels] + rate * np.exp(features[:, 0])\n",
    "        rotations = np.stack([np.cos(angles), -np.sin(angles), np.sin(angles), np.cos(angles)])\n",
    "        rotations = np.reshape(rotations.T, (-1, 2, 2))\n",
    "        x = .4 * np.random.permutation(np.einsum(\"ti,tij->tj\", features, rotations))\n",
    "\n",
    "        self.init_sample = torch.from_numpy(x).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tensor:\n",
    "        return self.init_sample[idx]\n",
    "\n",
    "\n",
    "data_size = 2 ** 15\n",
    "pinwheel_dataset = Pinwheel(data_size)\n",
    "checkerboard_dataset = Checkerboard(size=data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6656824",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 ** 10\n",
    "pinwheel_data_loader = DataLoader(pinwheel_dataset, batch_size, num_workers=0, pin_memory=True, shuffle=True)\n",
    "checkerboard_data_loader = DataLoader(checkerboard_dataset, batch_size, num_workers=0, pin_memory=True, shuffle=True)\n",
    "\n",
    "def show_2d_data(data: Tensor):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.scatter(data[:, 0], data[:, 1])\n",
    "    plt.xlim(-1.1, 1.1)\n",
    "    plt.ylim(-1.1, 1.1)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c6778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinwheel_batch = next(iter(pinwheel_data_loader))\n",
    "checkerboard_batch = next(iter(checkerboard_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_2d_data(next(iter(pinwheel_data_loader)))\n",
    "show_2d_data(next(iter(checkerboard_data_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db100b3e",
   "metadata": {},
   "source": [
    "$\\varepsilon$ 是布朗桥强度\n",
    "\n",
    "对于离散的情况\n",
    "\n",
    "从$x_0$和$x_1$的联合分布中采样$(x_0, x_1)$, 然后得出条件化的布朗桥\n",
    "\n",
    "$$\n",
    "\n",
    "p^{W^\\varepsilon}(x_{t_1}\\ldots x_{t_N} | x_0, x_1) = \\prod_{i=1}^{N} p^{W^\\varepsilon}(x_{t_i} | x_{t_{i-1}}, x_1) \\\\\n",
    "\n",
    "p^{W^\\varepsilon}(x_{t_i} | x_{t_{i-1}}, x_1) = \\mathcal{N}(x_{t_i}; x_{t_{i-1}} + \\frac{t_i - t_{i-1}}{1 - t_{i-1}}(x_1 - x_{t_{i-1}}),\\varepsilon \\frac{(t_i - t_{i-1})(1 - t_i)}{(1 - t_{i-1})} I)\\\\\n",
    "\n",
    "p^{W^\\varepsilon}(x_{t_{i}} | x_{t_{i+1}}, x_1) = \\mathcal{N}(x_{t_i}; \\frac{t_{i+1}-t_{i}}{t_{i+1}} x_0 + \\frac{t_{i}}{t_{i+1}} x_{t_{i+1}},\\varepsilon \\frac{t_i(t_{i+1} - t_{i})}{t_{i+1}} I)\\\\\n",
    "\n",
    "p^{W^\\varepsilon}(x_{t_N} | x_0, x_1) = \\mathcal{N}(x_{t_N}; (1 - t_N) x_0 + t_N x_1, \\varepsilon t_N(1 - t_N) I)\n",
    "\n",
    "$$\n",
    "\n",
    "MSBM训练:\n",
    "\n",
    "我们假定有N个均匀时间网格\n",
    "\n",
    "$$\n",
    "t_0 =0 \\lt t_1 \\lt \\cdots \\lt t_N = 1\n",
    "$$\n",
    "\n",
    "然后我们给定$m+1$个边界限定\n",
    "$$\n",
    "t_{k_0} = 0 \\lt t_{k_1} \\lt \\cdots \\lt t_{k_m} = 1, \n",
    "$$\n",
    "Let $N_j= k_{j+1}-k_j$.\n",
    "\n",
    "在第$j$段内，我们引入局部归一化时间:\n",
    "$$\n",
    "\\tau_i = \\frac{i}{N_j},\\quad i=0,\\ldots,N_j.\n",
    "$$\n",
    "这样在内部套用ASBM的布朗桥系数公式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f2b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentPosterior:\n",
    "    \"\"\"\n",
    "    一段 [k_j, k_{j+1}] 内部用到的局部布朗桥系数与便捷采样。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, N_j: int, epsilon: float, device):\n",
    "        self.N_j = N_j\n",
    "        self.epsilon = epsilon\n",
    "        t = torch.linspace(0, 1, N_j+1, device=device)  # 段内局部时间\n",
    "        self.posterior_mean_coef1 = 1 - t[:-1]/t[1:]    # for x_left\n",
    "        self.posterior_mean_coef2 = t[:-1]/t[1:]        # for x_{t+1}\n",
    "        var = epsilon * t[:-1]*(t[1:]-t[:-1]) / t[1:]\n",
    "        self.posterior_log_variance = torch.log(var.clamp(min=1e-20))\n",
    "        self.tau_tp1 = (torch.arange(N_j, device=device, dtype=torch.float32)+1.0)/float(N_j)\n",
    "\n",
    "    def sample_bridge_tp1(self, x_left, x_right, local_i):\n",
    "        \"\"\"采样 x_{t+1}（段内第 i->i+1 步的右端）\"\"\"\n",
    "        tau = self.tau_tp1[local_i].unsqueeze(-1)\n",
    "        mean = (1. - tau) * x_left + tau * x_right\n",
    "        var = self.epsilon * tau * (1. - tau)\n",
    "        noise = torch.randn_like(x_left)\n",
    "        mask = (tau>0) & (tau<1)\n",
    "        return mean + mask * torch.sqrt(var.clamp_min(1e-20)) * noise\n",
    "\n",
    "    def sample_posterior_t(self, x_left, x_tp1, local_i):\n",
    "        \"\"\"采样 x_t ~ q(x_t | x_{t+1}, x_left)（段内单步后验）\"\"\"\n",
    "        a = self.posterior_mean_coef1[local_i].unsqueeze(-1)  # coef for x_left\n",
    "        b = self.posterior_mean_coef2[local_i].unsqueeze(-1)  # coef for x_{t+1}\n",
    "        mean = a * x_left + b * x_tp1\n",
    "        logv = self.posterior_log_variance[local_i].unsqueeze(-1)\n",
    "        noise = torch.randn_like(x_tp1)\n",
    "        nonzero = (local_i > 0).float().unsqueeze(-1)\n",
    "        return mean + nonzero * torch.exp(0.5*logv) * noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529731e1",
   "metadata": {},
   "source": [
    "步数映射器，并预处理系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b793fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLayout:\n",
    "    \"\"\"\n",
    "    输入：N（总步数），锚点索引列表 k_list（含起终点：0 与 N）\n",
    "    输出：seg_id[t], local_i[t]，以及每段的系数\n",
    "    \"\"\"\n",
    "    def __init__(self, N:int, k_list:List[int], epsilon, device):\n",
    "        print(k_list[0], k_list[-1])\n",
    "        assert k_list[0]==0 and k_list[-1]==N\n",
    "        assert all(k_list[i]<k_list[i+1] for i in range(len(k_list)-1))\n",
    "        self.N = N\n",
    "        self.k_list = k_list\n",
    "        \n",
    "        # 第j段对应的时间区间\n",
    "        self.seg_ranges = [(k_list[j], k_list[j+1]) for j in range(len(k_list)-1)]\n",
    "        # 第j段对应的长度\n",
    "        self.seg_N = [b-a for (a,b) in self.seg_ranges]\n",
    "        # 预构建段内系数和内部布朗采样\n",
    "        self.seg_post = [SegmentPosterior(nj, epsilon, device) for nj in self.seg_N]\n",
    "        # 建立映射\n",
    "        seg_id = torch.empty(N, dtype=torch.long)\n",
    "        local_i = torch.empty(N, dtype=torch.long)\n",
    "        for j,(a,b) in enumerate(self.seg_ranges):\n",
    "            # 全局 t in [a, b-1] 对应段 j 的 i in [0, N_j-1]\n",
    "            seg_id[a:b] = j\n",
    "            local_i[a:b] = torch.arange(0, b-a, dtype=torch.long)\n",
    "        self.seg_id = seg_id.to(device)\n",
    "        self.local_i = local_i.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fb46a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    \"\"\"\n",
    "    Multimarginal 分段桥采样器：\n",
    "      - layout: TimeLayout(N, k_list, epsilon, device)\n",
    "      - 真样本：sample_pair_condition(anchors_tuple, t_idx) -> (x_t, x_tp1)\n",
    "      - 假样本：posterior_from_pred_left(x_left_pred, x_tp1, t_idx) -> x_t_fake\n",
    "                （把生成器预测的“该段左端点”当作 x_L）\n",
    "    \"\"\"\n",
    "    def __init__(self, layout: TimeLayout, device):\n",
    "        self.layout = layout\n",
    "        self.device = device\n",
    "\n",
    "    def sample_pair_condition(self, anchors_tuple, t_idx: torch.Tensor):\n",
    "        \"\"\"\n",
    "        真一步对 (x_t, x_{t+1})：\n",
    "          anchors_tuple: (x_{k0}, x_{k1}, ..., x_{km}), 每个 (B,D)\n",
    "          t_idx: (B,) int64, 全局 t ∈ {0,...,N-1}\n",
    "        返回:\n",
    "          x_t, x_tp1 皆为 (B,D)\n",
    "        \"\"\"\n",
    "        assert t_idx.dtype in (torch.int32, torch.int64), \"t_idx must be int tensor\"\n",
    "        N = self.layout.N\n",
    "        # 逐元素检查范围\n",
    "        if not torch.all((t_idx >= 0) & (t_idx < N)):\n",
    "            raise ValueError(\"t_idx out of valid range [0, N-1]\")\n",
    "\n",
    "        B = t_idx.size(0)\n",
    "        seg = self.layout.seg_id[t_idx]      # (B,)\n",
    "        loc = self.layout.local_i[t_idx]     # (B,)\n",
    "\n",
    "        # 选出每个样本所在段的左右锚点\n",
    "        # 注意：anchors_tuple[j][b] 按样本 b 取第 j 段的左/右锚点\n",
    "        xL = torch.stack([anchors_tuple[j][b]     for b, j in enumerate(seg.tolist())], dim=0).to(self.device)\n",
    "        xR = torch.stack([anchors_tuple[j+1][b]   for b, j in enumerate(seg.tolist())], dim=0).to(self.device)\n",
    "\n",
    "        x_t   = torch.empty_like(xL)\n",
    "        x_tp1 = torch.empty_like(xL)\n",
    "\n",
    "        # 分段路由\n",
    "        for j, post in enumerate(self.layout.seg_post):\n",
    "            mask = (seg == j)\n",
    "            if mask.any():\n",
    "                idx = mask.nonzero(as_tuple=False).squeeze(1)\n",
    "                loc_j = loc[idx]\n",
    "                x_tp1[idx] = post.sample_bridge_tp1(xL[idx], xR[idx], loc_j)\n",
    "                x_t[idx]   = post.sample_posterior_t(xL[idx], x_tp1[idx], loc_j)\n",
    "\n",
    "        return x_t, x_tp1\n",
    "    def step_std(self, t_idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        返回每个样本在其所在段、所在局部步 i 的一步标准差 σ_i = sqrt(eps * v_i)\n",
    "        形状：(B, 1)\n",
    "        \"\"\"\n",
    "        assert t_idx.dtype in (torch.int32, torch.int64)\n",
    "        N = self.layout.N\n",
    "        if not torch.all((t_idx >= 0) & (t_idx < N)):\n",
    "            raise ValueError(\"t_idx out of valid range [0, N-1]\")\n",
    "\n",
    "        seg = self.layout.seg_id[t_idx]      # (B,)\n",
    "        loc = self.layout.local_i[t_idx]     # (B,)\n",
    "        B = t_idx.size(0)\n",
    "        std = torch.empty(B, 1, device=self.device, dtype=torch.float32)\n",
    "\n",
    "        for j, post in enumerate(self.layout.seg_post):\n",
    "            mask = (seg == j)\n",
    "            if mask.any():\n",
    "                idx = mask.nonzero(as_tuple=False).squeeze(1)\n",
    "                # post.posterior_log_variance: (N_j,)\n",
    "                logv = post.posterior_log_variance[loc[idx]].unsqueeze(-1)  # (b_j,1)\n",
    "                std[idx] = torch.exp(0.5 * logv)\n",
    "        return std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f1dee0",
   "metadata": {},
   "source": [
    "接下来需要一个统一的DataLoader，返回多锚点配对数据，作为从联合分布q采样的工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13523cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAnchorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    返回一条多锚点样本 (x_k0, x_k1, ..., x_km)。\n",
    "    默认各锚点独立采样；如需更强耦合可后续替换为 minibatch OT。\n",
    "    \"\"\"\n",
    "    def __init__(self, dist_classes, dist_kwargs_list, size, pairing=\"independent\"):\n",
    "        assert len(dist_classes) == len(dist_kwargs_list)\n",
    "        self.dist_classes = dist_classes\n",
    "        self.dist_kwargs_list = dist_kwargs_list\n",
    "        self.size = size\n",
    "        self.pairing = pairing\n",
    "        self.regenerate()\n",
    "\n",
    "    def regenerate(self):\n",
    "        self.datasets = [C(size=self.size, **kw) for C,kw in zip(self.dist_classes, self.dist_kwargs_list)]\n",
    "        print(f\"Regenerated {len(self.datasets)} anchor datasets, size={self.size}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.pairing == \"aligned\":\n",
    "            ii = [idx % len(ds) for ds in self.datasets]\n",
    "        else:\n",
    "            ii = [random.randint(0, len(ds)-1) for ds in self.datasets]\n",
    "        return tuple(ds[i] for ds,i in zip(self.datasets, ii))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9d5546",
   "metadata": {},
   "source": [
    "接下来的训练，我们要能做到以下几个能力\n",
    "1. 给定$x_0, x_1\\sim q(x_0,x_1)$ （这里直接假设独立）, 构造 $(x_t, x_{t+1})$ 作为布朗桥ground truth, 先按$q(x_{t+1}| x_0, x_1)$采样$x_{t+1}$，再按$q(x_t|x_{t+1}, x_0)$采样$x_{t}$\n",
    "2. 生成器：给定$(x_t,t)$， 预测 $x_0$\n",
    "3. 判别器：给定$(x_t, x_{t+1})$，判断是不是由生成器生成的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb039d42",
   "metadata": {},
   "source": [
    "现在是判别器和生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self, x_dim, t_dim, n_t, out_dim, layers,\n",
    "        active=partial(nn.LeakyReLU, 0.2),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.x_dim = x_dim\n",
    "        self.t_dim = t_dim\n",
    "\n",
    "        self.model_list = []\n",
    "        ch_prev = x_dim + t_dim \n",
    "\n",
    "        self.t_transform = nn.Embedding(n_t, t_dim,)\n",
    "\n",
    "        for ch_next in layers:\n",
    "            self.model_list.append(nn.Linear(ch_prev, ch_next))\n",
    "            self.model_list.append(active())\n",
    "            ch_prev = ch_next\n",
    "\n",
    "        self.model_list.append(nn.Linear(ch_prev, out_dim))\n",
    "        self.model = nn.Sequential(*self.model_list)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        return self.model(\n",
    "            torch.cat([\n",
    "                x,\n",
    "                self.t_transform(t),\n",
    "            ], dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "class MyDiscriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self, x_dim, t_dim, n_t, layers,\n",
    "        active=partial(nn.LeakyReLU, 0.2),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.x_dim = x_dim\n",
    "        self.t_dim = t_dim\n",
    "\n",
    "        self.model_list = []\n",
    "        ch_prev = 2 * x_dim + t_dim\n",
    "\n",
    "        self.t_transform = nn.Embedding(n_t, t_dim,)\n",
    "        \n",
    "        for ch_next in layers:\n",
    "            # print(f\"Build layer from {ch_prev} to {ch_next}\")\n",
    "            self.model_list.append(nn.Linear(ch_prev, ch_next))\n",
    "            self.model_list.append(active())\n",
    "            ch_prev = ch_next\n",
    "\n",
    "        self.model_list.append(nn.Linear(ch_prev, 1))\n",
    "        self.model = nn.Sequential(*self.model_list)\n",
    "\n",
    "    def forward(self, x_t, t, x_tp1,):\n",
    "        transform_t = self.t_transform(t)\n",
    "        # print(f\"x_t.shape = {x_t.shape}, transform_t = {transform_t.shape}, x_tp1 = {x_tp1.shape}\")\n",
    "\n",
    "        return self.model(\n",
    "            torch.cat([\n",
    "                x_t,\n",
    "                transform_t,\n",
    "                x_tp1,\n",
    "            ], dim=1)\n",
    "        ).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de16e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c5bef8",
   "metadata": {},
   "source": [
    "必要的参数\n",
    "1. Batch size\n",
    "2. learning rate: lr_d, lr_g\n",
    "3. optimizer: Adam beta1, beta2\n",
    "4. epoch, 迭代次数\n",
    "5. epsilon, 布朗桥强度\n",
    "6. num_timesteps, 时间步数\n",
    "7. x_dim, 数据维度\n",
    "8. t_dim, 时间维度\n",
    "9. z_dim, 隐空间维度\n",
    "10. output_dir, 输出目录\n",
    "11. seed, 随机种子\n",
    "12. use_minibatch_ot, 是否使用minibatch OT\n",
    "13. use_r1, 是否使用r1正则化\n",
    "14. r1_gamma, r1正则化系数\n",
    "15. lazy_reg, r1正则化的间隔\n",
    "16. use_ema, 是否使用ema\n",
    "17. ema_decay, ema衰减系数\n",
    "18. save_ckpt, 是否保存模型\n",
    "19. ckpt_interval, 保存模型的间隔\n",
    "20. print, 是否打印训练情况\n",
    "21. print_interval, 打印训练情况的间隔\n",
    "22. vis, 是否可视化\n",
    "23. vis_interval, 可视化的间隔\n",
    "24. resume, 是否从断点继续训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d04515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def parse_args(usedefault:bool = False):\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch_size', type=int, default=2**10)\n",
    "    parser.add_argument('--lr_d', type=float, default=2e-4)\n",
    "    parser.add_argument('--lr_g', type=float, default=2e-4)\n",
    "    parser.add_argument('--beta1', type=float, default=0.5)\n",
    "    parser.add_argument('--beta2', type=float, default=0.9)\n",
    "    parser.add_argument('--epoch', type=int, default=1000)\n",
    "    parser.add_argument('--epsilon', type=float, default=0.25)\n",
    "    parser.add_argument('--x_dim', type=int, default=2)\n",
    "    parser.add_argument('--t_dim', type=int, default=6)\n",
    "    parser.add_argument('--num_timesteps', type=int, default=10)\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--output_dir', type=str, default='./output')\n",
    "    parser.add_argument('--use_minibatch_ot', action='store_true', help='whether to use minibatch OT coupling')\n",
    "    parser.add_argument('--use_r1', action='store_true', help='whether to use r1 regularization')\n",
    "    parser.add_argument('--r1_gamma', type=float, default=0.01, help='r1 regularization coefficient')\n",
    "    parser.add_argument('--lazy_reg', type=int, default=1, help='r1 regularization interval')\n",
    "    parser.add_argument('--use_ema', action='store_true', help='whether to use ema')\n",
    "    parser.add_argument('--ema_decay', type=float, default=0.999, help='ema decay coefficient')\n",
    "    parser.add_argument('--save_ckpt', action='store_true', help='whether to save model checkpoints')\n",
    "    parser.add_argument('--print', action='store_true', help='whether to print training progress')\n",
    "    parser.add_argument('--print_interval', type=int, default=100, help='interval for printing training progress')\n",
    "    parser.add_argument('--vis', action='store_true', help='whether to visualize training progress')\n",
    "    parser.add_argument('--vis_interval', type=int, default=1000, help='interval for visualizing training progress')\n",
    "    parser.add_argument('--resume', action='store_true', help='whether to resume training from a checkpoint')\n",
    "\n",
    "    if usedefault:\n",
    "        args = parser.parse_args([])\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "    \n",
    "    args.k_list = [0, 4 , 8, args.num_timesteps]\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bfa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(netG, sampler, args, anchors_tuple, iteration, device):\n",
    "    \"\"\"\n",
    "    多边界可视化：从最右锚点 x_{k_m} 出发，按 t = N-1 -> 0 逐步回滚到左端。\n",
    "    - netG: 生成器，输入 (x_{t+1}, t, z) 预测该段左端锚点\n",
    "    - sampler: Sampler(layout=TimeLayout(...))，用于分段一步后验\n",
    "    - anchors_tuple: (x_{k0}, x_{k1}, ..., x_{km})，每个 (B,D)\n",
    "    \"\"\"\n",
    "    N = sampler.layout.N\n",
    "    x_right_all = anchors_tuple[-1]                  # 取最右锚点分布样本 (B,D)\n",
    "    n_vis = min(4096, x_right_all.size(0))\n",
    "    x_vis = x_right_all[:n_vis].clone().to(device)   # 当前“处于 t=1”的点\n",
    "\n",
    "    fig, axes = plt.subplots(1, N+1, figsize=((N+1)*2, 3))\n",
    "    # 面板 0：t=1（最右端）\n",
    "    axes[0].scatter(x_vis[:, 0].cpu(), x_vis[:, 1].cpu(), s=2)\n",
    "    axes[0].set_title(\"t=1.00\")\n",
    "    axes[0].set_xlim(-1.1, 1.1); axes[0].set_ylim(-1.1, 1.1)\n",
    "\n",
    "    # 逐步回滚 t = N-1, ..., 0\n",
    "    for i in reversed(range(N)):\n",
    "        t_vis = torch.full((x_vis.size(0),), i, device=device, dtype=torch.long)\n",
    "        dx_pred = netG(x_vis, t_vis, z_vis)\n",
    "        std_vis = sampler.step_std(t_vis)\n",
    "        x_vis = x_vis + dx_pred + std_vis * torch.randn_like(x_vis)\n",
    "\n",
    "        col = N - i    # 第 col 个面板显示当前 x_t\n",
    "        axes[col].scatter(x_vis[:, 0].cpu(), x_vis[:, 1].cpu(), s=2)\n",
    "        axes[col].set_title(f\"t={i/float(N):.2f}\")\n",
    "        axes[col].set_xlim(-1.1, 1.1); axes[col].set_ylim(-1.1, 1.1)\n",
    "\n",
    "    plt.suptitle(f\"iter {iteration}\" + (\" (EMA)\" if args.use_ema else \"\"))\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(args.output_dir, f'vis_{iteration}.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa44ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    set_seed(args.seed)\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    N = int(args.num_timesteps)  \n",
    "    print(f\"N={N}\")\n",
    "    layout = TimeLayout(N, args.k_list, args.epsilon, device=device)\n",
    "    sampler = Sampler(layout=layout, device=device)\n",
    "    # ==== Models ====\n",
    "    netG = MyGenerator(\n",
    "        x_dim=args.x_dim,\n",
    "        t_dim=args.t_dim,\n",
    "        n_t=args.num_timesteps,\n",
    "        out_dim=args.x_dim,\n",
    "        layers=[256,256,256],\n",
    "    ).to(device)\n",
    "    netD = MyDiscriminator(\n",
    "        x_dim=args.x_dim,\n",
    "        t_dim=args.t_dim,\n",
    "        n_t=args.num_timesteps,\n",
    "        layers=[256,256,256],\n",
    "    ).to(device)\n",
    "\n",
    "    if args.use_ema:\n",
    "        from torch_ema import ExponentialMovingAverage\n",
    "        ema_g = ExponentialMovingAverage(netG.parameters(), decay=args.ema_decay)\n",
    "        ema_g.to(device)\n",
    "        print(f\"Using EMA with decay = {args.ema_decay}\")\n",
    "    else:\n",
    "        ema_g = None\n",
    "        print(\"Not using EMA\")\n",
    "\n",
    "    # ==== Opt & Sched ====\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=args.lr_d, betas=(args.beta1, args.beta2))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=args.lr_g, betas=(args.beta1, args.beta2))\n",
    "    schedulerD = optim.lr_scheduler.CosineAnnealingLR(optimizerD, T_max=args.epoch, eta_min=1e-6)\n",
    "    schedulerG = optim.lr_scheduler.CosineAnnealingLR(optimizerG, T_max=args.epoch, eta_min=1e-6)\n",
    "\n",
    "    # ==== (Optional) resume ====\n",
    "    init_iteration = 0\n",
    "    checkpoint_path = os.path.join(args.output_dir, 'checkpoint.pth')\n",
    "    if args.resume and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        netG.load_state_dict(checkpoint['netG_dict'])\n",
    "        optimizerG.load_state_dict(checkpoint['optimizerG'])\n",
    "        schedulerG.load_state_dict(checkpoint['schedulerG'])\n",
    "        netD.load_state_dict(checkpoint['netD_dict'])\n",
    "        optimizerD.load_state_dict(checkpoint['optimizerD'])\n",
    "        schedulerD.load_state_dict(checkpoint['schedulerD'])\n",
    "        init_iteration = checkpoint['iteration']\n",
    "        print(f\"=> loaded checkpoint (iteration {init_iteration})\")\n",
    "    else:\n",
    "        if args.resume:\n",
    "            print(f\"=> no checkpoint found at {checkpoint_path}, training from scratch\")\n",
    "        else:\n",
    "            print(f\"=> training from scratch\")\n",
    "\n",
    "    multi_ds = MultiAnchorDataset(\n",
    "        dist_classes=[Checkerboard, Checkerboard, Pinwheel, Pinwheel],  # 例子：交替分布\n",
    "        dist_kwargs_list=[{'grid_size':4}, {'grid_size':3}, {'num_classes':4}, {'num_classes':6}],\n",
    "        size=args.batch_size * 1000,\n",
    "        pairing=\"independent\",\n",
    "    )\n",
    "    sb_loader = DataLoader(multi_ds, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "    n_t = args.num_timesteps\n",
    "    # ==== utils ====\n",
    "    softplus = F.softplus\n",
    "\n",
    "    def r1_penalty(d_out, x_in, gamma=0.05):\n",
    "        grad = torch.autograd.grad(\n",
    "            outputs=d_out.sum(), inputs=x_in, create_graph=True,\n",
    "            retain_graph=True, only_inputs=True\n",
    "        )[0]\n",
    "        grad_penalty = (grad.view(grad.size(0), -1).norm(2, dim=1) ** 2).mean()\n",
    "        return 0.5 * gamma * grad_penalty\n",
    "\n",
    "    # ==== Train loop ====\n",
    "    iteration = init_iteration\n",
    "    print(\"Start training MSBM...\")\n",
    "    \n",
    "    for epoch in range(args.epoch):\n",
    "        for anchors in sb_loader:\n",
    "            netG.train(); netD.train()\n",
    "            anchors = tuple(a.to(device) for a in anchors)\n",
    "            B = anchors[0].size(0)\n",
    "\n",
    "            # （可选）批内 OT 重配\n",
    "            if args.use_minibatch_ot:\n",
    "                # TODO\n",
    "                raise NotImplementedError(\"Minibatch OT coupling is not implemented yet.\")\n",
    "            \n",
    "            #########################\n",
    "            # Discriminator training\n",
    "            #########################\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = True\n",
    "                \n",
    "            netD.zero_grad()\n",
    "\n",
    "            # 采样时间步 - D专用\n",
    "            t = torch.randint(0, n_t, (B,), device=device, dtype=torch.long)\n",
    "            \n",
    "            # 真对\n",
    "            x_t_real, x_tp1_real = sampler.sample_pair_condition(anchors, t)\n",
    "            x_t_real.requires_grad = True  # 用于R1\n",
    "            \n",
    "            D_real = netD(x_t_real, t, x_tp1_real.detach()).view(-1)\n",
    "            errD_real = softplus(-D_real).mean()\n",
    "\n",
    "            # R1正则化\n",
    "            r1_loss = torch.tensor(0., device=device)\n",
    "            if args.use_r1 and (iteration % args.lazy_reg == 0):\n",
    "                r1_loss = r1_penalty(D_real, x_t_real, gamma=args.r1_gamma)\n",
    "            \n",
    "            (errD_real + r1_loss).backward()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # 位移预测（不需要 detach x_tp1_real，这里反正 no_grad）\n",
    "                dx_pred = netG(x_tp1_real, t)\n",
    "                std = sampler.step_std(t)                        # (B,1)\n",
    "                noise = torch.randn_like(x_tp1_real)\n",
    "                x_t_fake = x_tp1_real + dx_pred + std * noise    # 关键：位移参数化\n",
    "            \n",
    "            D_fake = netD(x_t_fake, t, x_tp1_real.detach()).view(-1)\n",
    "            errD_fake = softplus(D_fake).mean()\n",
    "            errD_fake.backward()\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "            nn.utils.clip_grad_norm_(netD.parameters(), max_norm=1.0)\n",
    "            optimizerD.step()\n",
    "            \n",
    "            #########################\n",
    "            # Generator training\n",
    "            #########################\n",
    "            for p in netD.parameters():\n",
    "                p.requires_grad = False\n",
    "            \n",
    "            netG.zero_grad()\n",
    "\n",
    "            # 重新采样时间步\n",
    "            t = torch.randint(0, n_t, (B,), device=device, dtype=torch.long)\n",
    "            \n",
    "            # 重新生成训练对\n",
    "            _, x_tp1 = sampler.sample_pair_condition(anchors, t)\n",
    "            \n",
    "            dx_pred = netG(x_tp1.detach(), t)                 # 允许把条件 detach\n",
    "            std = sampler.step_std(t)\n",
    "            noise = torch.randn_like(x_tp1)                      # 噪声不需要梯度\n",
    "            x_t_fake = x_tp1 + dx_pred + std * noise\n",
    "\n",
    "            \n",
    "            output = netD(x_t_fake, t, x_tp1.detach()).view(-1)\n",
    "            errG = softplus(-output).mean()\n",
    "            errG.backward()\n",
    "            \n",
    "            # 梯度裁剪（可选）\n",
    "            nn.utils.clip_grad_norm_(netG.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizerG.step()\n",
    "\n",
    "            # ⭐ 更新EMA\n",
    "            if args.use_ema:\n",
    "                ema_g.update()\n",
    "            \n",
    "            iteration += 1\n",
    "            \n",
    "            if args.print and (iteration % args.print_interval == 0):\n",
    "                print(f\"[Iter {iteration}] D={errD.item():.4f} \"\n",
    "                      f\"(real {errD_real.item():.4f} fake {errD_fake.item():.4f}) | \"\n",
    "                      f\"G={errG.item():.4f}\")\n",
    "                \n",
    "            # 可视化（从源出发逆推到目标）\n",
    "            if args.vis and iteration % args.vis_interval == 0:\n",
    "                netG.eval()  # 设为eval模式\n",
    "                \n",
    "                # 如果使用EMA，用EMA权重；否则用当前权重\n",
    "                if args.use_ema:\n",
    "                    with ema_g.average_parameters():\n",
    "                        with torch.no_grad():\n",
    "                            visualize_samples(netG, sampler, args, anchors, iteration, device)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        visualize_samples(netG, sampler, args, anchors, iteration, device)\n",
    "                \n",
    "                netG.train()  # 恢复train模式\n",
    "\n",
    "\n",
    "        # 每个 epoch 调度 & 保存\n",
    "        schedulerD.step(); schedulerG.step()\n",
    "        # ⭐ 保存checkpoint（使用EMA权重）\n",
    "        if args.save_ckpt:\n",
    "            if args.use_ema:\n",
    "                with ema_g.average_parameters():\n",
    "                    content = {\n",
    "                        'iteration': iteration,\n",
    "                        'epoch': epoch,\n",
    "                        'netG_dict': netG.state_dict(),\n",
    "                        'optimizerG': optimizerG.state_dict(),\n",
    "                        'schedulerG': schedulerG.state_dict(),\n",
    "                        'netD_dict': netD.state_dict(),\n",
    "                        'optimizerD': optimizerD.state_dict(),\n",
    "                        'schedulerD': schedulerD.state_dict(),\n",
    "                    }\n",
    "            else:\n",
    "                content = {\n",
    "                    'iteration': iteration,\n",
    "                    'epoch': epoch,\n",
    "                    'netG_dict': netG.state_dict(),\n",
    "                    'optimizerG': optimizerG.state_dict(),\n",
    "                    'schedulerG': schedulerG.state_dict(),\n",
    "                    'netD_dict': netD.state_dict(),\n",
    "                    'optimizerD': optimizerD.state_dict(),\n",
    "                    'schedulerD': schedulerD.state_dict(),\n",
    "                }\n",
    "            \n",
    "            torch.save(content, os.path.join(args.output_dir, 'checkpoint.pth'))\n",
    "            print(f\"Checkpoint saved @ epoch {epoch+1}\")\n",
    "\n",
    "\n",
    "args = parse_args(usedefault=True)\n",
    "args.print = True\n",
    "args.vis = True\n",
    "args.save_ckpt = True\n",
    "args.use_ema = True\n",
    "args.resume = False\n",
    "print(args)\n",
    "train(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MSBM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
